{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH9WZ-qANDno",
        "outputId": "9e103495-a9b6-42b8-ee2e-3cbffb43da60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting the book scraping process...\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-1.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-2.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-3.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-4.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-5.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-6.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-7.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-8.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-9.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-10.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-11.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-12.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-13.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-14.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-15.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-16.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-17.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-18.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-19.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-20.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-21.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-22.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-23.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-24.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-25.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-26.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-27.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-28.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-29.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-30.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-31.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-32.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-33.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-34.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-35.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-36.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-37.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-38.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-39.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-40.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-41.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-42.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-43.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-44.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-45.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-46.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-47.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-48.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-49.html\n",
            "📡 Scraping page: http://books.toscrape.com/catalogue/page-50.html\n",
            "✅ Reached the last page.\n",
            "\n",
            "💾 Saving 1000 books to 'all_books_complete_data.csv'...\n",
            "   Save complete.\n",
            "\n",
            "--- 📊 Analysis Summary ---\n",
            "Total Books Scraped: 1000\n",
            "Average Book Price: £35.07\n",
            "Highest Rating Found: 5 stars\n",
            "Number of Books with Highest Rating: 196\n",
            "--------------------------\n",
            "\n",
            "🎉 Scraping process finished successfully!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_website():\n",
        "    print(\"🚀 Starting the book scraping process...\")\n",
        "\n",
        "    base_url = \"http://books.toscrape.com/catalogue/\"\n",
        "    start_page = \"page-1.html\"\n",
        "    output_csv_file = \"all_books_complete_data.csv\"\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'MyBookScraper/1.1 (Contact: my-email@example.com) - Educational Project'\n",
        "    }\n",
        "\n",
        "    all_scraped_books = []\n",
        "    rating_map = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
        "\n",
        "    try:\n",
        "        current_url = urljoin(base_url, start_page)\n",
        "\n",
        "        while current_url:\n",
        "            print(f\"📡 Scraping page: {current_url}\")\n",
        "\n",
        "            response = requests.get(current_url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            books_on_page = soup.find_all(\"article\", class_=\"product_pod\")\n",
        "            if not books_on_page:\n",
        "                print(\"⚠️ No books found on this page. Exiting loop.\")\n",
        "                break\n",
        "\n",
        "            for book in books_on_page:\n",
        "                title = book.h3.a[\"title\"]\n",
        "\n",
        "                price_str = book.find(\"p\", class_=\"price_color\").text\n",
        "                price = float(re.search(r\"[\\d.]+\", price_str).group())\n",
        "\n",
        "                rating_p_tag = book.find(\"p\", class_=re.compile(\"star-rating\"))\n",
        "                rating_text = rating_p_tag[\"class\"][1]\n",
        "                rating_num = rating_map.get(rating_text, 0)\n",
        "\n",
        "                all_scraped_books.append({\"title\": title, \"price\": price, \"rating\": rating_num})\n",
        "\n",
        "            next_li_element = soup.find(\"li\", class_=\"next\")\n",
        "            if next_li_element and next_li_element.a and next_li_element.a[\"href\"]:\n",
        "                next_page_relative_url = next_li_element.a[\"href\"]\n",
        "                current_url = urljoin(base_url, next_page_relative_url)\n",
        "                time.sleep(1)\n",
        "            else:\n",
        "                print(\"✅ Reached the last page.\")\n",
        "                current_url = None\n",
        "\n",
        "        print(f\"\\n💾 Saving {len(all_scraped_books)} books to '{output_csv_file}'...\")\n",
        "        with open(output_csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "            fieldnames = [\"title\", \"price\", \"rating\"]\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_scraped_books)\n",
        "        print(\"   Save complete.\")\n",
        "\n",
        "        if all_scraped_books:\n",
        "            total_price = sum(book[\"price\"] for book in all_scraped_books)\n",
        "            average_price = total_price / len(all_scraped_books)\n",
        "\n",
        "            max_rating = max(book[\"rating\"] for book in all_scraped_books)\n",
        "            best_rated_books = [book[\"title\"] for book in all_scraped_books if book[\"rating\"] == max_rating]\n",
        "\n",
        "            print(\"\\n--- 📊 Analysis Summary ---\")\n",
        "            print(f\"Total Books Scraped: {len(all_scraped_books)}\")\n",
        "            print(f\"Average Book Price: £{average_price:.2f}\")\n",
        "            print(f\"Highest Rating Found: {max_rating} stars\")\n",
        "            print(f\"Number of Books with Highest Rating: {len(best_rated_books)}\")\n",
        "            print(\"--------------------------\\n\")\n",
        "\n",
        "        print(\"🎉 Scraping process finished successfully!\")\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        print(f\"❌ HTTP Error occurred: {e}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"❌ A network error occurred: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ An unexpected error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_website()"
      ]
    }
  ]
}